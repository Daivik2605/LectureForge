version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: lectureforge-ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0 # Force Ollama to listen to the Docker network
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: lectureforge-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: lectureforge-backend
    ports:
      - "8000:8000"
    environment:
      - PYTHONPATH=/app
      - OLLAMA_BASE_URL=http://ollama:11434  # FIXED: Point to the service name 'ollama'
      - OLLAMA_MODEL=llama3.1:8b
      - LOG_LEVEL=INFO
      - ENVIRONMENT=production
      - CORS_ORIGINS=["http://localhost:3000","http://frontend:3000"]
      - REDIS_URL=redis://redis:6379/0
    volumes:
      - ./data:/app/backend/data
      - ./storage:/app/backend/storage
      - ./logs:/app/backend/logs
    depends_on:
      - ollama
      - redis
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: lectureforge-frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000 # For browser-side calls
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  ollama_data:
  redis_data:

networks:
  default:
    name: lectureforge-network
